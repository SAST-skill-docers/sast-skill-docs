## 0. å¼•è¨€

> æ‚¨æ˜¯å¦åŒå€¦äº†äººç±»ä¹‹é—´ä½¿ç”¨ç¦»æ•£è¯­è¨€çš„ä½æ•ˆäº¤æµï¼Ÿæ‚¨æ˜¯å¦å¸Œæœ›å­¦ä¹ æ›´åŠ æŠ½è±¡ä¸”é«˜æ•ˆçš„è¡¨è¾¾æ–¹å¼ï¼Œæˆ–è€…æ¸´æœ›ä¸è®¡ç®—æœºå»ºç«‹æ›´åŠ è¿ç»­çš„æ²Ÿé€šï¼Ÿæœ¬é—¨è¯¾ç¨‹å°†ä»è‡ªç„¶è¯­è¨€ä¸­å•è¯çš„æœ€åŸºæœ¬è¡¨ç¤ºâ€”â€”è¯å‘é‡å¼€å§‹è®²èµ·ï¼Œè¯¦ç»†è®²è§£å½“å‰ $ NLP$ ( $Natural Language Processing$, è‡ªç„¶è¯­è¨€å¤„ç†) çš„ä¸»æµæ¨¡å‹  $Transformer$ï¼Œå¹¶æ•™æˆå¦‚ä½•ä½¿ç”¨ç›®å‰æœ€çƒ­é—¨çš„ $NLP$ æ¨¡å‹æ‰˜ç®¡å¹³å° $Hugging Face$ ğŸ¤—ã€‚

>å¦‚æœæ‚¨è§‰å¾—è‡ªå·±çš„è¯­è¨€è¡¨è¾¾è¿‡äºç¦»æ•£ï¼Œå¸Œæœ›å­¦ä¹ å…·æœ‰è¿ç»­æ€§çš„è¡¨è¾¾æ–¹å¼ï¼›æˆ–è€…è®¤ä¸ºè‡ªå·±çš„è¡¨è¾¾è¿‡äºç›´ç™½ï¼Œç¼ºä¹é«˜å±‚æ¬¡çš„æŠ½è±¡ï¼›æˆ–è€…å¸Œæœ›åœ¨æœ¬åœ°éƒ¨ç½²ç±» $ChatGPT$ è¯­è¨€æ¨¡å‹ï¼Œé‚£ä¹ˆè¯·å°†æœ¬è¯¾ç¨‹å‹å…¥æ‚¨çš„è¿è¡Œæ ˆã€‚æœ¬è¯¾ç¨‹è™½æ— ç›Šäºæå‡æ‚¨ä½œä¸ºäººçš„è¡¨è¾¾èƒ½åŠ›ï¼Œå³æ•™ä¼šæ‚¨ä¸ªäººä½¿ç”¨è¯å‘é‡è¡¨è¾¾å¹¶å¯¹é«˜å±‚æ¬¡æŠ½è±¡è¯­ä¹‰ç‰¹å¾è¿›è¡Œæå–ï¼Œä½†å¯ä»¥ä½¿æ‚¨çš„æ¨¡å‹å…·æœ‰æ›´åŠ å®Œå¤‡çš„è‡ªç„¶è¯­è¨€ç†è§£ä¸è‡ªç„¶è¯­è¨€ç”Ÿæˆèƒ½åŠ›ï¼Œä»è€Œåœ¨è¯­è¨€ç±»ä»»åŠ¡ä¸­å®ç°å¯¹æ‚¨çš„ä»£æ›¿ã€‚ åœ¨æœ¬è¯¾ç¨‹ä¸­ï¼Œæ‚¨ï¼ˆæˆ–è€…æ‚¨çš„æ¨¡å‹ï¼Œå¦‚æœæ¯”æ‚¨æ›´æ™ºèƒ½çš„è¯ï¼‰å¯ä»¥ï¼š å­¦ä¹ å˜å½¢é‡‘åˆšçš„åŸºæœ¬åŸç†ã€ç»“æ„ï¼Œäº²æ‰‹ç»„è£…å¹¶è¿è¡Œæ‚¨çš„ç¬¬ä¸€ä¸ªå˜å½¢é‡‘åˆšï¼ˆæ¨¡å—æ— éœ€è‡ªå¤‡ï¼‰ã€‚äº†è§£å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒã€å¾®è°ƒæ–¹æ³•ã€‚ ç†Ÿç»ƒæŒæ¡æŠ±æŠ±è„¸çš„ä½¿ç”¨åœºæ™¯ä¸æŠ€å·§ã€‚ å¯¹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå®Œæˆç®€å•çš„æ–‡æœ¬ä»»åŠ¡ã€‚åœ¨å­¦ä¹ å®Œè®¡ç®—æœºè§†è§‰è¯¾ç¨‹åï¼Œå°è¯•ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹å®Œæˆæ–‡æœ¬ & å›¾åƒä»»åŠ¡ã€‚

## 1. å¦‚ä½•è¡¨ç¤ºä¸€ä¸ªè¯çš„å«ä¹‰ï¼Ÿ

- ç”¨ä¸€ä¸ª $id$ è¡¨ç¤ºä¸€ä¸ªè¯ï¼Ÿ

<img src="../static/ai-ml/nlp/1-id.png" style="display: block; margin: 0 auto; width: 50%;">



- ç¦»æ•£çš„ $id$ å‹è¯è¯­å¦‚ä½•è¾“å…¥åˆ°ç¥ç»ç½‘ç»œä¹‹ä¸­ï¼Ÿ
- ç¥ç»ç½‘ç»œçš„è¾“å…¥å¯¹è±¡é€šå¸¸æ˜¯ $feature$ -> æˆ‘ä»¬éœ€è¦å¾—åˆ°è¯çš„ $feature$xw
xw
### ğŸ¤—Word Embedding è¯åµŒå…¥

- å°†ä¸€ä¸ªè¯æ˜ å°„ä¸ºä¸€ä¸ª $ğ‘’ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘” \; dim$ ç»´çš„å‘é‡
- æ¯ä¸€ç»´å…·æœ‰ä¸€å®šçš„å«ä¹‰ï¼ˆå…·ä½“å«ä¹‰å¯èƒ½å¾ˆæŠ½è±¡ï¼‰


<img src="../static/ai-ml/nlp/2-embedding.png" style="display: block; margin: 0 auto; width: 90%;">

### ğŸ¤—å¦‚ä½•è·å– Word Embedding ï¼Ÿ

- æ‰‹å·¥æ„é€ ï¼Ÿ

  â€¢ â€œåŒ—äº¬åœ¨åŒ—çº¬ $40$ åº¦ï¼Œæ‰€ä»¥å¿…é¡»æœ‰ä¸€ç»´å‘é‡çš„å€¼æ˜¯ $40$ â€ï¼Ÿ

- é€šè¿‡åœ¨ä»»åŠ¡ä¸­æ‹Ÿåˆæ•°æ®è·å¾—

  â€¢ ä¾‹å¦‚å¯¹äº $Next\;token\;prediction$ ï¼ˆæ ¹æ®å‰ç¼€é¢„æµ‹ä¸‹ä¸ªè¯ï¼‰çš„ä»»åŠ¡ï¼š

  1. éšæœºåˆå§‹åŒ–æ‰€æœ‰è¯ $v$ çš„ $Word\; Embedding$ $E_v$
  2. å°†å‰ç¼€çš„ $Embedding$ æ±‚å’Œå–å¹³å‡ $E_{\text{predict}} = \frac{1}{k} \sum_{j=0}^{k-1} E_{v_j}$
  3. ä¸‹â¼€ä¸ªè¯æ˜¯ $Embedding$ ä¸ $E_{predict}$ æœ€æ¥è¿‘çš„è¯  $v_{\text{predict}} = \max_{v_i} \cos \left( E_{\text{predict}}, E_{v_i} \right)$
  4. è®¡ç®— $loss$ï¼Œåå‘ä¼ æ’­

## 2. å¦‚ä½•è·å–è¯åœ¨å¥å­ä¸­çš„å‘é‡è¡¨ç¤ºï¼Ÿ

>An Apple a Day Keeps the Doctor Away 

>ä¸€å¤©ä¸€éƒ¨ Iphone è®©æˆ‘ä¸åšå£«å­¦ä½å¤±ä¹‹äº¤è‡‚ï¼Ÿ


ä¸€ä¸ªç®€å•çš„æƒ³æ³•ï¼šå°†å¥ä¸­æ‰€æœ‰è¯çš„å‘é‡åŠ æƒæ±‚å’Œï¼Œè¡¨ç¤ºè¯åœ¨å¥ä¸­çš„å«ä¹‰ã€‚

### ğŸ¤—Skip-Gram & CBOW

ä¸€ä¸ªè¯çš„æ„æ€å–å†³äºç›¸é‚»è¯

<img src="../static/ai-ml/nlp/3-cbow.png" style="display: block; margin: 0 auto; width: 100%;">

### ğŸ¤—TextCNN

ä½¿ç”¨å·ç§¯æ¥èšåˆç‰¹å¾

<img src="../static/ai-ml/nlp/4-cnn.png" style="display: block; margin: 0 auto; width: 100%;">

### ğŸ¤—RNN

é¢„æµ‹ä¸‹ä¸€ä¸ªè¯

<img src="../static/ai-ml/nlp/5-rnn.png" style="display: block; margin: 0 auto; width: 100%;">

### ğŸ¤—é•¿è·ç¦»ä¾èµ–ï¼Ÿ

<img src="../static/ai-ml/nlp/6-distance.png" style="display: block; margin: 0 auto; width: 100%;">

### ğŸ¤—Attention is all you need!

<img src="../static/ai-ml/nlp/7-probs.png" style="display: block; margin: 0 auto; width: 100%;">

### ğŸ¤—Attention Probs

- å¦‚ä½•è·å– $ğ‘ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›\;ğ‘ğ‘Ÿğ‘œğ‘ğ‘ $ï¼Ÿä¸¤ä¸ªè¯è¶Šç›¸å…³è¶Šå¤§ï¼Ÿ

- ä»¥ä¸¤ä¸ªè¯ $ğ¸ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘”$ çš„ä½™å¼¦å¤¹è§’è¡¨ç¤ºç›¸ä¼¼åº¦ï¼š

$$\text{attention scores}(v_i, v_j) = \cos \left( E_{v_i}, E_{v_j} \right)$$

- ä½¿ç”¨ $ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥$ å½’ä¸€åŒ–ï¼š

$$\text{attention probs}(v_i, v_j) = \frac{e^{\text{attention scores}(v_i, v_j)}}{\sum_{k=0}^{\text{sequence length}} e^{\text{attention scores}(v_i, v_k)}}$$

$$ E_{v_iin\;sentence}= \sum_{j=0}^{\text{seq\;len}} \text{attention probs}(v_i, v_j) E_j$$


### ğŸ¤—å¦‚ä½•ç§‘å­¦è·å–Attention Scoreï¼Ÿ

- $ğ¸ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘”$ çš„ä½™å¼¦å¤¹è§’ï¼šä¸ $ğ‘£_{i}$ ç›¸ä¼¼çš„ç‰¹å¾çœŸçš„æ˜¯ $ğ‘£_{i}$ éœ€è¦çš„å—ï¼ŸğŸ¤”

  â€¢ å‡å¦‚å¥å­ä¸ºï¼šğŸğŸğŸğŸğŸğŸğŸğŸğŸğŸğŸare good.

  â€¢ä¸ $ğ¸_{ğŸ}$ æœ€æ¥è¿‘çš„æ€»æ˜¯ $ğ¸_{ğŸ}$ï¼Œ$ğ¸_{ğŸin\;sentence}$= $ğ¸_{ğŸ}$



- $ğ¸ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘” â†’ (ğ‘„ğ‘¢ğ‘’ğ‘Ÿğ‘¦,ğ¾ğ‘’ğ‘¦)$ ğŸ¤—ğŸ¤—ğŸ¤—

- $ğ‘„ğ‘¢ğ‘’ğ‘Ÿğ‘¦$ï¼šæƒ³æŸ¥è¯¢çš„ç‰¹å¾

- $ğ¾ğ‘’ğ‘¦$ï¼šæƒ³è¢«æŸ¥è¯¢çš„ç‰¹å¾

$$ 
\text{attention scores}(v_i, v_j) = \cos \left( E_{v_i}, E_{v_j} \right)
$$

### ğŸ¤” Attention

$$
\text{Embedding}\xrightarrow{\text{Linear}} (\text{Query, Key, Value})
$$

$$ 
\text{attention scores}(v_i, v_j) = \cos \left( E_{v_i}, E_{v_j} \right)
$$

$$ \text{attention probs}(v_i, v_j) = \frac{e^{\text{attention scores}(v_i, v_j)}}{\sum_{k=0}^{\text{sequence length}} e^{\text{attention scores}(v_i, v_k)}}  $$

$$ V_{v_iin sentence}  = \sum_{j=0}^{\text{seq len}} \text{attention probs}(v_i, v_j) V_j$$

$$E_{v_iin sentence}   \xleftarrow{\text{Linear}} V_{v_iin sentence} $$

### ğŸ¤— Attention

- $Query$: æˆ‘æƒ³æ‰¾åˆ°ä»–çš„åå­—â€¦ 

- $Key$: 

  â€¢ å°æ˜ * ä»–çš„åå­— = 0.8 

  â€¢ ä»– * ä»–çš„åå­— = 0.1 

  â€¢ å°åšç‰©å­¦å®¶ * ä»–çš„åå­— = 0.05 

  â€¢ â€¦ 

- $Result$ = 0.8 * $V_{å°æ˜}$ + 0.1 * $V_{ä»–}$ + 0.05 * $V_{å°åšç‰©å­¦å®¶}$ + â€¦

### ğŸ¤— Attention

- $\text{Input: }  \text{Embeddings}_{\quad \text{shape} = (\text{seq len}, \text{embedding dim})}$
- $Q = Q_{\text{proj}}(\text{Embeddings})_{\quad \text{shape} = (\text{seq len}, \text{query dim})}$ 

- $K = K_{\text{proj}}(\text{Embeddings}) _{\quad \text{shape} = (\text{seq len}, \text{key dim}), \; \text{key dim} = \text{query dim}}$

- $V = V_{\text{proj}}(\text{Embeddings}) _{\quad \text{shape} = (\text{seq len}, \text{value dim}) }$

- $\text{Attention Probs}(Q, K) = \text{softmax}\left(\frac{QK^T}{\sqrt{\text{d}}}\right) _{\quad \text{shape} = (\text{seq len}, \text{seq len})}$

- $\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{\text{d}}}\right) V _{\quad \text{shape} = (\text{seq len}, \text{value dim}) }\\$

- $\text{Output} = O_{\text{proj}}(\text{Attention}(Q, K, V)) _{\quad \text{shape} = (\text{seq len}, \text{embedding dim})}$
  

<img src="../static/ai-ml/nlp/8-dotproduct.png" style="display: block; margin: 0 auto; width: 30%;">



### ğŸ¤—å¦‚ä½•åŠ å…¥ä½ç½®ä¿¡æ¯ğŸ§­?

- $Attention$ æœºåˆ¶ä¸­ï¼Œå¹¶æ²¡æœ‰è€ƒè™‘å‘é‡åœ¨ $sequence$â€‹ ä¸­çš„ä½ç½®ã€‚

- $ğ‘ƒğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› ğ‘’ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘”$ï¼šä¸ºæ¯ä¸€ä¸ªä½ç½®é¢„è®¾ä¸€ä¸ªå‘é‡ $ğ‘ƒğ¸_{pos}$â€‹

  $$E_{v_iinput} = E_{v_i} + PE_{pos}$$

## 3. å¦‚ä½•åœ¨æ¨¡å‹ä¸­å­˜å‚¨çŸ¥è¯†ï¼Ÿ

### ğŸ¤—æ¨¡å‹ä¸­çŸ¥è¯†å­˜åœ¨ä½•å¤„ï¼Ÿ

- çŸ¥è¯†ï¼š$Key-Value Pair:  (K, V)$

- $Neural Memory$ï¼Œä½¿ç”¨ $x$  æŸ¥è¯¢ $k_i$ï¼š

$$p(k_i|x) \propto e^{k_i x}$$

$$MN(x) = \sum_{i=1}^{\text{dim}} p(k_i|x) \cdot v_i$$

$$K = [k_i], V = [v_i]$$

$$MN(x) = \text{softmax}(xK^T)V$$

- ç”¨ä¸¤ä¸ªçº¿æ€§å±‚å®ç°å‰é¦ˆç¥ç»ç½‘ç»œï¼š

  $$FFN(x) = f(xK^T)V$$

### ğŸ¤— Feedforward Neural Network

$$FFN(x) = f(xK^T)V$$

- $ğ‘“$ ä¸€èˆ¬ä½¿ç”¨ $ğ‘Ÿğ‘’ğ‘™ğ‘¢$ æˆ–å…¶å˜ç§
- ä¸€èˆ¬ $inner\;â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘› \;dim$ = $4\;ğ‘’ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘”\;dim$
- å‚æ•°é‡ï¼š$8Ã—ğ‘’ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘”ğ‘ \;dim^2$

### ğŸ¤— Transformer

- ä» $Input$ åºåˆ—åˆ° $Output$ åºåˆ—â€”â€”$Seq2Seq$

## 4. å¦‚ä½•åŸºäºä»¥ä¸ŠåŸç†æ„å»ºè¯­è¨€æ¨¡å‹ï¼Ÿ

- è¯­è¨€æ¨¡å‹ï¼š$P_\theta (\text{w|context})$

- $ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡$ï¼šä¸Šä¸‹æ–‡

- $w$ï¼šæŸä¸ªä½ç½®ä¸Šçš„æŸä¸ªè¯çš„æ¦‚ç‡

- è‡ªç¼–ç è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ $BERT$ï¼‰ï¼š

  â€¢ â€œåŒ—äº¬åœ¨åŒ—çº¬$[MASK]$åº¦â€ï¼Œæ±‚$[MASK]$å¡«è¯çš„æ¦‚ç‡åˆ†å¸ƒï¼šğ‘ƒ(â€¢|åŒ—äº¬åœ¨åŒ—çº¬$[MASK]$åº¦) 

- è‡ªç¼–ç è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ $GPT$ï¼‰: 

  â€¢ â€œåŒ—äº¬åœ¨åŒ—â€ï¼Œæ±‚ä¸‹ä¸€ä¸ªå­—çš„æ¦‚ç‡åˆ†å¸ƒï¼šğ‘ƒ(â€¢|åŒ—äº¬åœ¨åŒ—)

### ğŸ¤— Encoder Only

- ä» $Input$ åºåˆ—åˆ°åˆ†ç±»è¾“å‡ºâ€”â€” $Classification$
- å…¸å‹æ¨¡å‹ï¼š$BERT$

<img src="../static/ai-ml/nlp/10-bert.png" style="display: block; margin: 0 auto; width: 50%;">

### ğŸ¤— Decoder Only

- ä»è¾“å…¥åºåˆ—åˆ°è¾“å…¥åºåˆ—çš„å»¶é•¿â€”â€” $Generation $
- å…¸å‹æ¨¡å‹ï¼š$GPT$

<img src="../static/ai-ml/nlp/11-gpt.png" style="display: block; margin: 0 auto; width: 50%;">

### ğŸ¤— Attention Types

- $Self Attention$: $QKV$ æ¥è‡ªåŒä¸€åºåˆ—

<img src="../static/ai-ml/nlp/12-self.png" style="display: block; margin: 0 auto; width: 90%;">

- $Cross Attention$: $QKV$ æ¥è‡ªä¸åŒåºåˆ—
<img src="../static/ai-ml/nlp/13-cross.png" style="display: block; margin: 0 auto; width: 100%;">

- ä¸€ç§ç‰¹æ®Šçš„ $Self Attention$

<img src="../static/ai-ml/nlp/14-special.png" style="display: block; margin: 0 auto; width: 100%;">

### ğŸ¤— Multi Head Attention ğŸ™‚ğŸ™‚ğŸ™‚

- $\ (Q_0, Q_1, Q_2, Q_3) = Q = Q_{\text{proj}}(\text{Embeddings}) \quad Q_i.\ \text{shape} = (\text{seq len}, \frac{\text{embeddings dim}}{\text{num head}}) \\$â€‹
- $\ (K_0, K_1, K_2, K_3) = K = K_{\text{proj}}(\text{Embeddings}) \quad K_i.\ \text{shape} = (\text{seq len}, \frac{\text{embeddings dim}}{\text{num head}}) $â€‹
- $\ (V_0, V_1, V_2, V_3) = V = V_{\text{proj}}(\text{Embeddings}) \quad V_i.\ \text{shape} = (\text{seq len}, \frac{\text{embeddings dim}}{\text{num head}})$â€‹



- $\text{Output} = O_{\text{proj}}(\text{Concat}(\text{Attention}(Q_i, K_i, V_i)))$â€‹



- å‚æ•°é‡ï¼š$\ 4 \times \text{embeddings dim}^2$â€‹
- ä¸åŒ $Head$ å…³æ³¨ä¸åŒçš„è¯­ä¹‰å…³ç³»





<img src="../static/ai-ml/nlp/15-multihead.png" style="display: block; margin: 0 auto; width: 40%;">

### ğŸ¤— Group Query Attention

<img src="../static/ai-ml/nlp/16-groupquery.png" style="display: block; margin: 0 auto; width: 100%;">

### ğŸ¤— Transformer

- $LM Head$ï¼šä¸€ä¸ªçº¿æ€§å±‚ï¼Œè¾“å…¥ç»´åº¦ä¸º $embedding dim$ï¼›è¾“å‡ºç»´åº¦ä¸ºè¯è¡¨å¤§å° $vocab\;size$ã€‚
- è¾“å‡ºæ¯ä¸ªä½ç½®ä¸Šå„è¯æœªå½’ä¸€åŒ–çš„å¯¹æ•°æ¦‚ç‡ logits: 
$$
P_\theta (\cdot \mid \text{context}) = \text{softmax}(\text{logits})
$$
- å·¦ä¾§ä¸º $Encoder$
- å³ä¾§ä¸º $Decoder$


<img src="../static/ai-ml/nlp/9-transformer.png" style="display: block; margin: 0 auto; width: 100%;">


### ğŸ¤— Encoder

- $BERT$
- ä¸€æ¬¡å‰å‘ä¼ æ’­å³å¯è®¡ç®—å‡ºæ‰€æœ‰$[MASK]$çš„æ¦‚ç‡åˆ†å¸ƒ
- æ–‡æœ¬ç†è§£ä»»åŠ¡

<img src="../static/ai-ml/nlp/17-encoder.png" style="display: block; margin: 0 auto; width: 100%;">

### ğŸ¤— Decoder

- $GPT$
- è‡ªå›å½’ç”Ÿæˆï¼šæ¯æ¬¡ç”Ÿæˆä¸‹ä¸€ä¸ªè¯
- $Masked Attention$ï¼šæ¯ä¸ªè¯åš$Attention$æ—¶åªèƒ½åŠ æƒåˆ°ä¹‹å‰çš„è¯
- æ–‡æœ¬ç”Ÿæˆä»»åŠ¡

<img src="../static/ai-ml/nlp/18-decoder.png" style="display: block; margin: 0 auto; width: 100%;">

### ğŸ¤—å¦‚ä½•è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Ÿ

- éšæœºåˆå§‹åŒ–æƒé‡ğœƒ 

- åå¤éšæœºåˆå§‹åŒ–æƒé‡ğœƒï¼Œç›´åˆ°æƒé‡èƒ½è¾ƒå¥½åœ°å®Œæˆä»»åŠ¡âŒ 

- ä½¿ç”¨å¤§é‡äººå·¥æ ‡æ³¨æ–‡æœ¬å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒğŸ˜§ğŸ˜§ğŸ˜§ 

- ä½¿ç”¨å¤§é‡æ— æ ‡æ³¨æ–‡æœ¬å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒğŸ§ğŸ˜‰ğŸ¤—

### ğŸ¤—é¢„è®­ç»ƒ

- $Mask filling or Next token prediction?$

- $Mask filling$:

  â€¢ æˆ‘æ˜¯$[MASK]$ï¼Œ$[MASK]$è€ƒè¯•æ²¡æœ‰ä¸€æ¬¡$[MASK]$ã€‚

$$\max_\theta \left( P_\theta (\text{å¤§å­¦ç”Ÿ|context}) \cdot P_\theta (\text{å¹¼å„¿å›­|context} \cdot P_\theta (\text{å‚åŠ |context}) \right)$$

- $Next token prediction$:

  â€¢ æˆ‘æ˜¯å¤§å­¦ç”Ÿï¼Œ

$$\max_\theta \left( \prod P_\theta (\text{next token} \mid \text{prefix}) \right)
  = \max_\theta \left( P_\theta (\text{å¤§å­¦ç”Ÿ|æˆ‘æ˜¯}) \cdot P_\theta (\text{, |æˆ‘æ˜¯å¤§å­¦ç”Ÿ}) \right)
$$

### ğŸ¤—å¦‚ä½•ä½¿ç”¨è¯­è¨€æ¨¡å‹å®Œæˆå…·ä½“ä¸‹æ¸¸ä»»åŠ¡ï¼Ÿ

- åå¤éšæœºåˆå§‹åŒ–æƒé‡ğœƒï¼Œç›´åˆ°æƒé‡ğœƒèƒ½è¾ƒå¥½åœ°å®Œæˆä»»åŠ¡âŒ 
- åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šç»§ç»­ä½¿ç”¨å¤§é‡æ— æ ‡æ³¨æ–‡æœ¬è®­ç»ƒğŸ˜Ÿ 
- ä½¿ç”¨å°‘é‡æ ‡æ³¨æ•°æ® $(ğ‘¥,ğ‘¦)$ æ¨¡å‹è¿›è¡Œè®­ç»ƒğŸ¤—ğŸ¤—ğŸ¤—

#### ä¾‹ï¼šåˆ†ç±»ä»»åŠ¡

- æ›¿æ¢é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„ $LM Head$ ä¸º $Classification Head $

- $Classification Head$ è¾“å…¥ç»´åº¦ä¸º $embedding\,dim$

- è¾“å‡ºç»´åº¦ä¸ºç±»åˆ«æ•°ç›® $num\,labels$

- ~~ç„¶åç›´æ¥æ‹¿å»éƒ¨ç½²~~

  

- ä½¿ç”¨æ ‡æ³¨çš„æ•°æ® $(ğ‘¥,ğ‘¦)$ è¿›è¡Œè®­ç»ƒ

  â€¢ $x$:æ–‡æœ¬ï¼›$y$:ç±»åˆ«

### ğŸ¤—å¦‚ä½•ä½¿ç”¨æœ‰é™çš„ç¡¬ä»¶èµ„æºè¿›è¡Œå¾®è°ƒï¼Ÿ

- $LoRA: Low-Rank Adaptation$

<img src="../static/ai-ml/nlp/19-lora.png" style="display: block; margin: 0 auto; width: 100%;">

### ğŸ¤—å¦‚ä½•è®©è¯­è¨€æ¨¡å‹æ›´å¥½çš„ç†è§£äººç±»æ„å›¾ï¼Ÿ

- æŒ‡ä»¤å¾®è°ƒ
- ä½¿ç”¨äººå·¥æ ‡æ³¨çš„ ($prompt, response$) å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒ
- å¼ºåŒ–å­¦ä¹ 

<img src="../static/ai-ml/nlp/20-finetune.png" style="display: block; margin: 0 auto; width: 100%;">